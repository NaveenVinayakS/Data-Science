{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "20\n",
      "5.0\n",
      "5.666666666666667\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "print(2+2)\n",
    "print(50-5*6)\n",
    "print((50-5*6)/4)\n",
    "print(17/3)\n",
    "print(5**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n",
      "thon\n",
      "Py\n",
      "Python\n",
      "Python\n"
     ]
    }
   ],
   "source": [
    "word='Python'\n",
    "print(word[-1])\n",
    "print(word[2:])\n",
    "print(word[:2])\n",
    "print(word[:2] + word[2:])\n",
    "print(word[:4] + word[4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nython'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'N'+word[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyPy\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(word[:2] + 'Py')\n",
    "print(len(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List=[1,3,4,5,6]\n",
    "List[0]\n",
    "List[-5]\n",
    "List[2:]\n",
    "List[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 4, 5, 6, 7, 8, 9, 0, 1]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List+[7,8,9,0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List updation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 8, 27, 64, 125]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cubes = [1, 8, 27, 65, 125]\n",
    "cubes[3]=64\n",
    "cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 8, 27, 64, 125, 216]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cubes.append(6**3)\n",
    "cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 8, 27, 64, 125, 216, 343, 512, 729]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cubes.extend([7**3,8**3,9**3])\n",
    "cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d', 'e']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'D', 'E', 'f']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters=['a','b','c','d','e','f']\n",
    "print(letters[3:5])\n",
    "letters[3:5]=['D','E']\n",
    "letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'D', 'g', 'h', 'i']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters[4:6]=[]\n",
    "letters\n",
    "print(len(letters))\n",
    "\n",
    "letters.extend(['g','h','i'])\n",
    "letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a', 'b', 'c'], [1, 2, 3]]\n",
      "[['a', 'b', 'c', 1, 2, 3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=['a','b','c']\n",
    "n=[1,2,3]\n",
    "v=[a,n]\n",
    "print(v)\n",
    "h=[a+n]\n",
    "print(h)\n",
    "\n",
    "#Slicing\n",
    "v[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Flow Control </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi 2\n",
      "bye 3\n",
      "to 2\n",
      "we 2\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "word=['hi','bye','to','we']\n",
    "for i in word:\n",
    "    print(i,len(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 hi\n",
      "1 bye\n",
      "2 to\n",
      "3 we\n"
     ]
    }
   ],
   "source": [
    "for w in range(len(word)):\n",
    "    print(w,word[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "print(sum(range(4)))\n",
    "print(list(range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 is a prime number\n",
      "3 is a prime number\n",
      "4 equals 2 * 2\n",
      "5 is a prime number\n",
      "6 equals 2 * 3\n",
      "7 is a prime number\n",
      "8 equals 2 * 4\n",
      "9 equals 3 * 3\n"
     ]
    }
   ],
   "source": [
    "for n in range(2, 10):\n",
    "    for x in range(2, n):\n",
    "        if n % x == 0:\n",
    "            print(n, 'equals', x, '*', n//x)\n",
    "            break\n",
    "    else:\n",
    "        print(n, 'is a prime number')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_incrementor(n):\n",
    "    return lambda x : x + n\n",
    "\n",
    "f = make_incrementor(53)\n",
    "f(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'cherry']\n",
      "['cherry', 'banana', 'apple']\n"
     ]
    }
   ],
   "source": [
    "fruits = ['apple', 'banana', 'cherry']\n",
    "print(fruits)\n",
    "fruits.reverse()\n",
    "print(fruits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cherry', 'gova', 'banana', 'apple']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruits.insert(1,'gova')\n",
    "fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq=[]\n",
    "for i in range(1,10):\n",
    "    sq.append(i**2)\n",
    "sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "squ=lambda x : x ** 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squ(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NESTED LIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = [\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [9, 10, 11, 12],\n",
    "]\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[row[i] for row in matrix] for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=[]\n",
    "for i in range(4):\n",
    "    t.append([row[i] for row in matrix])       \n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TUPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10\n",
      "1 20\n",
      "2 30\n",
      "3 40\n",
      "4 50\n"
     ]
    }
   ],
   "source": [
    "T = 10,20,30,40,50\n",
    "\n",
    "for i in T:\n",
    "    print(T.index(i),i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10\n",
      "1 20\n",
      "2 30\n",
      "3 40\n",
      "4 50\n"
     ]
    }
   ],
   "source": [
    "for var in range(len(T)):\n",
    "    print(var,T[var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set - set is an unordered collection with no duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b', 'f', 'g', 'a', 'e', 't', 'j'}\n"
     ]
    }
   ],
   "source": [
    "hi={'hi','h','j'}\n",
    "len(hi)\n",
    "a=set('afgbjtefg')\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cherry', 'gova', 'banana', 'apple']"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "k=dict({1:'d',2:'c'})\n",
    "print(len(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 gova\n",
      "2 banana\n"
     ]
    }
   ],
   "source": [
    "for i in k.keys():\n",
    "    print(i,fruits[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geeks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 'welcome', 2: 'c', 3: (2, 3, 4), 5: {'Nested': {1: 'Life', 2: 'Geeks'}}}"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding a value in Dict\n",
    "k[3]=2,3,4\n",
    "#updating dictionary\n",
    "k[1]='welcome'\n",
    "#updating with nested dict\n",
    "k[5] = {'Nested' :{1 : 'Life', 2 : 'Geeks'}} \n",
    "print(k[5]['Nested'][2])\n",
    "#deleting in dictionary\n",
    "#del k[5]\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-230-88433b40a3a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#f=lambda x , y :k[x], k[5]['Nested'][2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Nested'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#f=lambda x , y :k[x], k[5]['Nested'][2]\n",
    "\n",
    "f=lambda x , y :k[x] , k[y]['Nested'][2]\n",
    "f(3)(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = lambda x, y : x + y\n",
    "g(76,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#printing in reversed manner\n",
    "for i in sorted(range(0,11)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple\n",
      "banana\n",
      "orange\n",
      "pear\n"
     ]
    }
   ],
   "source": [
    "basket = ['apple', 'orange', 'apple', 'pear', 'orange', 'banana']\n",
    "for i in sorted(set(basket)):\n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 2]\n",
      "[0, 2, 2]\n",
      "[0, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# REMOVE - removes the first matching value, not a specific index:\n",
    "a = [0, 2, 3, 2]\n",
    "a.remove(2)\n",
    "print(a)\n",
    "\n",
    "# DELETE (DEL) - removes the item at a specific index:\n",
    "\n",
    "b = [0,2,3,2]\n",
    "del b[2]\n",
    "print(b)\n",
    "\n",
    "# POP - removes the item at a specific index and returns it:\n",
    "\n",
    "c = [0,2,3,2]\n",
    "c.pop(3)\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>NLTK NLP</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "para=\"\"\"Undercomplete Autoencoders, Regularized Autoencoders, Representational Power, Layer size and Depth, Stochastic Encoders and Decoders, Denoising Autoencoders, Learning Manifolds with autoencoders, Contractive Autoencoders, Predictive Sparse decomposition, and Applications of Autoencoders\"\"\"\n",
    "\n",
    "sentence = nltk.sent_tokenize(para)\n",
    "word = nltk.word_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "senetence  : -  ['Undercomplete Autoencoders, Regularized Autoencoders, Representational Power, Layer size and Depth, Stochastic Encoders and Decoders, Denoising Autoencoders, Learning Manifolds with autoencoders, Contractive Autoencoders, Predictive Sparse decomposition, and Applications of Autoencoders']\n",
      "\n",
      "word       : -  ['Undercomplete', 'Autoencoders', ',', 'Regularized', 'Autoencoders', ',', 'Representational', 'Power', ',', 'Layer', 'size', 'and', 'Depth', ',', 'Stochastic', 'Encoders', 'and', 'Decoders', ',', 'Denoising', 'Autoencoders', ',', 'Learning', 'Manifolds', 'with', 'autoencoders', ',', 'Contractive', 'Autoencoders', ',', 'Predictive', 'Sparse', 'decomposition', ',', 'and', 'Applications', 'of', 'Autoencoders']\n"
     ]
    }
   ],
   "source": [
    "print(\"senetence  : - \",sentence)\n",
    "print(\"\\nword       : - \",word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>stop words </b> : of,from,other,and,our,them,then etc. these kind of words are repeated again and again , and this word does not play a better role when we are solving different kind of application with respect\n",
    "o positive and negative sentiment analysis\n",
    "they dont specify much value in sentences. stop words removes these kind of words\n",
    "<b>stopwords.words('english')</b> - to see list of stop words in english\n",
    "\n",
    "https://www.quora.com/What-is-difference-between-stemming-and-lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I have three  visions for India,The sun is a huge ball of gases history people,\\nit has a diameter of 1,392,000 km.', 'It is so huge that it can hold millions of planets inside it.', 'The Sun is mainly made up of hydrogen and helium gas.', 'The surface of the Sun is known as the photosphere.', 'The photosphere is surrounded by a thin layer of gas known as the chromospheres.', 'Without the Sun, there would be no life on Earth.', 'There would be no plants, no animals and no human beings.', 'As, all the living things on Earth get their energy from the Sun for their survival.']\n",
      "\n",
      " sentences loop : - I have three  visions for India,The sun is a huge ball of gases history people,\n",
      "it has a diameter of 1,392,000 km.\n",
      "\n",
      " words in paragraph :-  ['I', 'have', 'three', 'visions', 'for', 'India', ',', 'The', 'sun', 'is', 'a', 'huge', 'ball', 'of', 'gases', 'history', 'people', ',', 'it', 'has', 'a', 'diameter', 'of', '1,392,000', 'km', '.']\n",
      "printing stemmer : -  I\n",
      "printing stemmer : -  three\n",
      "printing stemmer : -  vision\n",
      "printing stemmer : -  india\n",
      "printing stemmer : -  ,\n",
      "printing stemmer : -  the\n",
      "printing stemmer : -  sun\n",
      "printing stemmer : -  huge\n",
      "printing stemmer : -  ball\n",
      "printing stemmer : -  gase\n",
      "printing stemmer : -  histori\n",
      "printing stemmer : -  peopl\n",
      "printing stemmer : -  ,\n",
      "printing stemmer : -  diamet\n",
      "printing stemmer : -  1,392,000\n",
      "printing stemmer : -  km\n",
      "printing stemmer : -  .\n",
      "\n",
      " sentences loop : - It is so huge that it can hold millions of planets inside it.\n",
      "\n",
      " words in paragraph :-  ['It', 'is', 'so', 'huge', 'that', 'it', 'can', 'hold', 'millions', 'of', 'planets', 'inside', 'it', '.']\n",
      "printing stemmer : -  It\n",
      "printing stemmer : -  huge\n",
      "printing stemmer : -  hold\n",
      "printing stemmer : -  million\n",
      "printing stemmer : -  planet\n",
      "printing stemmer : -  insid\n",
      "printing stemmer : -  .\n",
      "\n",
      " sentences loop : - The Sun is mainly made up of hydrogen and helium gas.\n",
      "\n",
      " words in paragraph :-  ['The', 'Sun', 'is', 'mainly', 'made', 'up', 'of', 'hydrogen', 'and', 'helium', 'gas', '.']\n",
      "printing stemmer : -  the\n",
      "printing stemmer : -  sun\n",
      "printing stemmer : -  mainli\n",
      "printing stemmer : -  made\n",
      "printing stemmer : -  hydrogen\n",
      "printing stemmer : -  helium\n",
      "printing stemmer : -  ga\n",
      "printing stemmer : -  .\n",
      "\n",
      " sentences loop : - The surface of the Sun is known as the photosphere.\n",
      "\n",
      " words in paragraph :-  ['The', 'surface', 'of', 'the', 'Sun', 'is', 'known', 'as', 'the', 'photosphere', '.']\n",
      "printing stemmer : -  the\n",
      "printing stemmer : -  surfac\n",
      "printing stemmer : -  sun\n",
      "printing stemmer : -  known\n",
      "printing stemmer : -  photospher\n",
      "printing stemmer : -  .\n",
      "\n",
      " sentences loop : - The photosphere is surrounded by a thin layer of gas known as the chromospheres.\n",
      "\n",
      " words in paragraph :-  ['The', 'photosphere', 'is', 'surrounded', 'by', 'a', 'thin', 'layer', 'of', 'gas', 'known', 'as', 'the', 'chromospheres', '.']\n",
      "printing stemmer : -  the\n",
      "printing stemmer : -  photospher\n",
      "printing stemmer : -  surround\n",
      "printing stemmer : -  thin\n",
      "printing stemmer : -  layer\n",
      "printing stemmer : -  ga\n",
      "printing stemmer : -  known\n",
      "printing stemmer : -  chromospher\n",
      "printing stemmer : -  .\n",
      "\n",
      " sentences loop : - Without the Sun, there would be no life on Earth.\n",
      "\n",
      " words in paragraph :-  ['Without', 'the', 'Sun', ',', 'there', 'would', 'be', 'no', 'life', 'on', 'Earth', '.']\n",
      "printing stemmer : -  without\n",
      "printing stemmer : -  sun\n",
      "printing stemmer : -  ,\n",
      "printing stemmer : -  would\n",
      "printing stemmer : -  life\n",
      "printing stemmer : -  earth\n",
      "printing stemmer : -  .\n",
      "\n",
      " sentences loop : - There would be no plants, no animals and no human beings.\n",
      "\n",
      " words in paragraph :-  ['There', 'would', 'be', 'no', 'plants', ',', 'no', 'animals', 'and', 'no', 'human', 'beings', '.']\n",
      "printing stemmer : -  there\n",
      "printing stemmer : -  would\n",
      "printing stemmer : -  plant\n",
      "printing stemmer : -  ,\n",
      "printing stemmer : -  anim\n",
      "printing stemmer : -  human\n",
      "printing stemmer : -  be\n",
      "printing stemmer : -  .\n",
      "\n",
      " sentences loop : - As, all the living things on Earth get their energy from the Sun for their survival.\n",
      "\n",
      " words in paragraph :-  ['As', ',', 'all', 'the', 'living', 'things', 'on', 'Earth', 'get', 'their', 'energy', 'from', 'the', 'Sun', 'for', 'their', 'survival', '.']\n",
      "printing stemmer : -  As\n",
      "printing stemmer : -  ,\n",
      "printing stemmer : -  live\n",
      "printing stemmer : -  thing\n",
      "printing stemmer : -  earth\n",
      "printing stemmer : -  get\n",
      "printing stemmer : -  energi\n",
      "printing stemmer : -  sun\n",
      "printing stemmer : -  surviv\n",
      "printing stemmer : -  .\n",
      "['I', 'three', 'vision', 'india', ',', 'the', 'sun', 'huge', 'ball', 'gase', 'histori', 'peopl', ',', 'diamet', '1,392,000', 'km', '.', 'It', 'huge', 'hold', 'million', 'planet', 'insid', '.', 'the', 'sun', 'mainli', 'made', 'hydrogen', 'helium', 'ga', '.', 'the', 'surfac', 'sun', 'known', 'photospher', '.', 'the', 'photospher', 'surround', 'thin', 'layer', 'ga', 'known', 'chromospher', '.', 'without', 'sun', ',', 'would', 'life', 'earth', '.', 'there', 'would', 'plant', ',', 'anim', 'human', 'be', '.', 'As', ',', 'live', 'thing', 'earth', 'get', 'energi', 'sun', 'surviv', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "paragraph = \"\"\"I have three  visions for India,The sun is a huge ball of gases history people,\n",
    "it has a diameter of 1,392,000 km.\n",
    "It is so huge that it can hold millions of planets inside it.\n",
    "The Sun is mainly made up of hydrogen and helium gas.\n",
    "The surface of the Sun is known as the photosphere.\n",
    "The photosphere is surrounded by a thin layer of gas known as the chromospheres.\n",
    "Without the Sun, there would be no life on Earth. There would be no plants, no animals and no human beings.\n",
    "As, all the living things on Earth get their energy from the Sun for their survival.\"\"\"\n",
    "\n",
    "# always convert paragraph to sentences.\n",
    "sentence = nltk.sent_tokenize(paragraph)\n",
    "print(sentence)\n",
    "\n",
    "#word = nltk.word_tokenize(sentence[i])\n",
    "#print(\"checking word\",word)\n",
    "# PorterStemmer for stemming purpose\n",
    "\n",
    "stemmer=PorterStemmer()\n",
    "\n",
    "lis=[]\n",
    "# converting sentence into words and then stemming it\n",
    "\n",
    "for i in sentence:\n",
    "    print(\"\\n sentences loop : -\",i)\n",
    "    words = nltk.word_tokenize(i)\n",
    "    print(\"\\n words in paragraph :- \",words)\n",
    "    for j in words:\n",
    "        if j not in set(stopwords.words('english')):\n",
    "            st= stemmer.stem(j)\n",
    "            #print(\"stemmed words with no stop words :- \\n\",st)\n",
    "            print(\"printing stemmer : - \",st)\n",
    "            lis.append(st)\n",
    "            #sentence[i] = ' '.join(j)\n",
    "\n",
    "print(lis)          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem with <b>stemming</b> is that words created after stemming will not have any meaning. in order to overcome this technique\n",
    "there we use <b>Lemmatization</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I three vision india , the sun huge ball ga histori peopl , diamet 1,392,000 km .\n",
      "It huge hold million planet insid .\n",
      "the sun mainli made hydrogen helium ga .\n",
      "the surfac sun known photospher .\n",
      "the photospher surround thin layer ga known chromospher .\n",
      "without sun , would life earth .\n",
      "there would plant , anim human .\n",
      "As , live thing earth get energi sun surviv .\n"
     ]
    }
   ],
   "source": [
    "#this works same as above code \n",
    "for i in range(len(sentence)):\n",
    "    words = nltk.word_tokenize(sentence[i])\n",
    "    #print(words)\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    k=sentence[i] = ' '.join(words)\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I three vision India , The sun huge ball gas history people , diameter 1,392,000 km .\n",
      "It huge hold million planet inside .\n",
      "The Sun mainly made hydrogen helium gas .\n",
      "The surface Sun known photosphere .\n",
      "The photosphere surrounded thin layer gas known chromosphere .\n",
      "Without Sun , would life Earth .\n",
      "There would plant , animal human being .\n",
      "As , living thing Earth get energy Sun survival .\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "\n",
    "#in stemming the words created after stemming will not have any meaning. in order to overcome this technique\n",
    "#there we use lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for i in range(len(sentence)):\n",
    "    words = nltk.word_tokenize(sentence[i])\n",
    "    #print(words)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    k=sentence[i] = ' '.join(words)\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence loop :- I have three  visions for India,The sun is a huge ball of gases history people,\n",
      "it has a diameter of 1,392,000 km.\n",
      "\n",
      " words in paragraph :-  ['I', 'have', 'three', 'visions', 'for', 'India', ',', 'The', 'sun', 'is', 'a', 'huge', 'ball', 'of', 'gases', 'history', 'people', ',', 'it', 'has', 'a', 'diameter', 'of', '1,392,000', 'km', '.']\n",
      "Printing Lemmatized word :- I\n",
      "Printing Lemmatized word :- three\n",
      "Printing Lemmatized word :- vision\n",
      "Printing Lemmatized word :- India\n",
      "Printing Lemmatized word :- ,\n",
      "Printing Lemmatized word :- The\n",
      "Printing Lemmatized word :- sun\n",
      "Printing Lemmatized word :- huge\n",
      "Printing Lemmatized word :- ball\n",
      "Printing Lemmatized word :- gas\n",
      "Printing Lemmatized word :- history\n",
      "Printing Lemmatized word :- people\n",
      "Printing Lemmatized word :- ,\n",
      "Printing Lemmatized word :- diameter\n",
      "Printing Lemmatized word :- 1,392,000\n",
      "Printing Lemmatized word :- km\n",
      "Printing Lemmatized word :- .\n",
      "sentence loop :- It is so huge that it can hold millions of planets inside it.\n",
      "\n",
      " words in paragraph :-  ['It', 'is', 'so', 'huge', 'that', 'it', 'can', 'hold', 'millions', 'of', 'planets', 'inside', 'it', '.']\n",
      "Printing Lemmatized word :- It\n",
      "Printing Lemmatized word :- huge\n",
      "Printing Lemmatized word :- hold\n",
      "Printing Lemmatized word :- million\n",
      "Printing Lemmatized word :- planet\n",
      "Printing Lemmatized word :- inside\n",
      "Printing Lemmatized word :- .\n",
      "sentence loop :- The Sun is mainly made up of hydrogen and helium gas.\n",
      "\n",
      " words in paragraph :-  ['The', 'Sun', 'is', 'mainly', 'made', 'up', 'of', 'hydrogen', 'and', 'helium', 'gas', '.']\n",
      "Printing Lemmatized word :- The\n",
      "Printing Lemmatized word :- Sun\n",
      "Printing Lemmatized word :- mainly\n",
      "Printing Lemmatized word :- made\n",
      "Printing Lemmatized word :- hydrogen\n",
      "Printing Lemmatized word :- helium\n",
      "Printing Lemmatized word :- gas\n",
      "Printing Lemmatized word :- .\n",
      "sentence loop :- The surface of the Sun is known as the photosphere.\n",
      "\n",
      " words in paragraph :-  ['The', 'surface', 'of', 'the', 'Sun', 'is', 'known', 'as', 'the', 'photosphere', '.']\n",
      "Printing Lemmatized word :- The\n",
      "Printing Lemmatized word :- surface\n",
      "Printing Lemmatized word :- Sun\n",
      "Printing Lemmatized word :- known\n",
      "Printing Lemmatized word :- photosphere\n",
      "Printing Lemmatized word :- .\n",
      "sentence loop :- The photosphere is surrounded by a thin layer of gas known as the chromospheres.\n",
      "\n",
      " words in paragraph :-  ['The', 'photosphere', 'is', 'surrounded', 'by', 'a', 'thin', 'layer', 'of', 'gas', 'known', 'as', 'the', 'chromospheres', '.']\n",
      "Printing Lemmatized word :- The\n",
      "Printing Lemmatized word :- photosphere\n",
      "Printing Lemmatized word :- surrounded\n",
      "Printing Lemmatized word :- thin\n",
      "Printing Lemmatized word :- layer\n",
      "Printing Lemmatized word :- gas\n",
      "Printing Lemmatized word :- known\n",
      "Printing Lemmatized word :- chromosphere\n",
      "Printing Lemmatized word :- .\n",
      "sentence loop :- Without the Sun, there would be no life on Earth.\n",
      "\n",
      " words in paragraph :-  ['Without', 'the', 'Sun', ',', 'there', 'would', 'be', 'no', 'life', 'on', 'Earth', '.']\n",
      "Printing Lemmatized word :- Without\n",
      "Printing Lemmatized word :- Sun\n",
      "Printing Lemmatized word :- ,\n",
      "Printing Lemmatized word :- would\n",
      "Printing Lemmatized word :- life\n",
      "Printing Lemmatized word :- Earth\n",
      "Printing Lemmatized word :- .\n",
      "sentence loop :- There would be no plants, no animals and no human beings.\n",
      "\n",
      " words in paragraph :-  ['There', 'would', 'be', 'no', 'plants', ',', 'no', 'animals', 'and', 'no', 'human', 'beings', '.']\n",
      "Printing Lemmatized word :- There\n",
      "Printing Lemmatized word :- would\n",
      "Printing Lemmatized word :- plant\n",
      "Printing Lemmatized word :- ,\n",
      "Printing Lemmatized word :- animal\n",
      "Printing Lemmatized word :- human\n",
      "Printing Lemmatized word :- being\n",
      "Printing Lemmatized word :- .\n",
      "sentence loop :- As, all the living things on Earth get their energy from the Sun for their survival.\n",
      "\n",
      " words in paragraph :-  ['As', ',', 'all', 'the', 'living', 'things', 'on', 'Earth', 'get', 'their', 'energy', 'from', 'the', 'Sun', 'for', 'their', 'survival', '.']\n",
      "Printing Lemmatized word :- As\n",
      "Printing Lemmatized word :- ,\n",
      "Printing Lemmatized word :- living\n",
      "Printing Lemmatized word :- thing\n",
      "Printing Lemmatized word :- Earth\n",
      "Printing Lemmatized word :- get\n",
      "Printing Lemmatized word :- energy\n",
      "Printing Lemmatized word :- Sun\n",
      "Printing Lemmatized word :- survival\n",
      "Printing Lemmatized word :- .\n"
     ]
    }
   ],
   "source": [
    "#this works same as above code\n",
    "\n",
    "\n",
    "lis=[]\n",
    "# converting sentence into words and then stemming it\n",
    "\n",
    "for i in sentence:\n",
    "    print(\"sentence loop :-\",i)\n",
    "    words = nltk.word_tokenize(i)\n",
    "    print(\"\\n words in paragraph :- \",words)\n",
    "    for j in words:\n",
    "        if j not in set(stopwords.words('english')):\n",
    "            Lem= lemmatizer.lemmatize(j)\n",
    "            #print(\"stemmed words with no stop words :- \\n\",st)\n",
    "            print(\"Printing Lemmatized word :-\",Lem)\n",
    "            lis.append(Lem)\n",
    "            #sentence[i] = ' '.join(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentiment analysis usually happen on +ve and -ve words, so stop words must be removed in order to find the sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words :- is a document matrix(counts number of times a word is present) (converts sentence into vectors)\n",
    "\n",
    "1. lower the sentences (no capital letter)\n",
    "2. apply stemming and Lemmatization\n",
    "3. apply stop words\n",
    "4. apply Bag Of Words \n",
    "\n",
    "Disadvantages :-\n",
    "\n",
    "we cant find  or assign weight to words in Bag Of words, we have to know the important word in a sentence which cannot be found in Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['three vision india sun huge ball gas history people diameter km',\n",
       " 'huge hold million planet inside',\n",
       " 'sun mainly made hydrogen helium gas',\n",
       " 'surface sun known photosphere',\n",
       " 'photosphere surrounded thin layer gas known chromosphere',\n",
       " 'without sun would life earth',\n",
       " 'would plant animal human being',\n",
       " 'living thing earth get energy sun survival']"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaning of text :- lowering words, removing stop words, lemmatization/stemming\n",
    "\n",
    "paragraph = \"\"\"I have three  visions for India,The sun is a huge ball of gases history people,\n",
    "it has a diameter of 1,392,000 km.\n",
    "It is so huge that it can hold millions of planets inside it.\n",
    "The Sun is mainly made up of hydrogen and helium gas.\n",
    "The surface of the Sun is known as the photosphere.\n",
    "The photosphere is surrounded by a thin layer of gas known as the chromospheres.\n",
    "Without the Sun, there would be no life on Earth. There would be no plants, no animals and no human beings.\n",
    "As, all the living things on Earth get their energy from the Sun for their survival.\"\"\"\n",
    "\n",
    "\n",
    "import re # regular expression library\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# stemming\n",
    "ps = PorterStemmer()\n",
    "# Lemmatization\n",
    "wordnet = WordNetLemmatizer()\n",
    "#sentence\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "corpus=[]\n",
    "\n",
    "#removing punctuation from sentences\n",
    "for i in range(len(sentence)):\n",
    "    \n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i]) #except the letters and space remove punctuations\n",
    "    review = review.lower() # changing capital to small\n",
    "    review = review.split() # split of sentence , now review will have list of words\n",
    "    #review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))] #stemming \n",
    "    review = [lemmatizer.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review) #joining the words\n",
    "    corpus.append(review) # appending the list\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "       [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating Bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF\n",
    "\n",
    "TF-IDF (counts number of times a word is present) (converts sentence into vectors) (weightage of words will be calculated)\n",
    "\n",
    "<b>TF</b> = No.of repeatation of words in a sentence / No.of words in a sentence (only 1 sentence)     \n",
    "<b>IDF</b> (Inverse Document Frequency) = log(No. of sentence / No. of sentences containig the word)\n",
    "Miltiply TF with IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.32375505, 0.        , 0.        , 0.32375505,\n",
       "        0.        , 0.        , 0.23413725, 0.        , 0.        ,\n",
       "        0.32375505, 0.        , 0.271332  , 0.        , 0.        ,\n",
       "        0.32375505, 0.        , 0.32375505, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.32375505, 0.        , 0.        , 0.        , 0.1817142 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.32375505, 0.32375505, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.46114911, 0.38647895, 0.        , 0.        ,\n",
       "        0.        , 0.46114911, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.46114911,\n",
       "        0.        , 0.        , 0.46114911, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.32879075, 0.        , 0.45463788,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.45463788,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.45463788, 0.45463788, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.25517489,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.50818054, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.50818054, 0.        , 0.        , 0.34033443,\n",
       "        0.60636422, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.41072846, 0.        ,\n",
       "        0.        , 0.        , 0.29703578, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.34422251, 0.41072846,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.34422251, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.41072846, 0.        , 0.41072846, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.4345364 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.51849157, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.29101409,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.51849157, 0.4345364 ],\n",
       "       [0.46114911, 0.        , 0.46114911, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.46114911, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.46114911, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.38647895],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.34164891, 0.40765763, 0.        , 0.40765763, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.40765763, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.22880626,\n",
       "        0.        , 0.        , 0.40765763, 0.        , 0.40765763,\n",
       "        0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph = \"\"\"I have three visions for India,The sun is a huge ball of gases history people,\n",
    "it has a diameter of 1,392,000 km.\n",
    "It is so huge that it can hold millions of planets inside it.\n",
    "The Sun is mainly made up of hydrogen and helium gas.\n",
    "The surface of the Sun is known as the photosphere.\n",
    "The photosphere is surrounded by a thin layer of gas known as the chromospheres.\n",
    "Without the Sun, there would be no life on Earth. There would be no plants, no animals and no human beings.\n",
    "As, all the living things on Earth get their energy from the Sun for their survival.\"\"\"\n",
    "\n",
    "\n",
    "import re # regular expression library\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# stemming\n",
    "ps = PorterStemmer()\n",
    "# Lemmatization\n",
    "wordnet = WordNetLemmatizer()\n",
    "#sentence\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "corpus = []\n",
    "\n",
    "#removing punctuation from sentences\n",
    "for i in range(len(sentence)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i]) #except the letters and space remove punctuations\n",
    "    review = review.lower() # changing capital to small\n",
    "    review = review.split() # split of sentence , now review will have list of words\n",
    "    #review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))] #stemming \n",
    "    review = [lemmatizer.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review) #joining the words\n",
    "    corpus.append(review) # appending the list\n",
    "\n",
    "    \n",
    "# importing TF-IDF and fitting it\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(corpus).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spam Classifier - Project\n",
    "\n",
    "DataSet\n",
    "https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
    "\n",
    "we can even use Lemmatization insted of stemming and insted of BagOfWords CountVectorization we can use TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "messages = pd.read_csv('/home/dpuser/Python_notebook/smsspamcollection/SMSSpamCollection', sep='\\t' , names=['Label', 'Message'])\n",
    "\n",
    "\n",
    "# cleaning the data set\n",
    "import re # regular expression library\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "corpus=[]\n",
    "\n",
    "#removing punctuation from sentences\n",
    "for i in range(0, len(messages)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['Message'][i]) #except the letters replace everything with space\n",
    "    review = review.lower() # changing capital to small\n",
    "    review = review.split() # split of sentence , now review will have list of words\n",
    "    \n",
    "    #review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))] #stemming \n",
    "    review = [lemmatizer.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    \n",
    "    review = ' '.join(review) #joining the words\n",
    "    corpus.append(review) # appending the list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     ham\n",
       "1     ham\n",
       "2    spam\n",
       "3     ham\n",
       "4     ham\n",
       "Name: Label, dtype: object"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['Label'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ham  spam\n",
      "0    1     0\n",
      "1    1     0\n",
      "2    0     1\n",
      "3    1     0\n",
      "4    1     0\n",
      "\n",
      " confusion Matrix \n",
      " [[942  13]\n",
      " [  8 152]]\n",
      " \n",
      " accuracy 0.9811659192825112\n"
     ]
    }
   ],
   "source": [
    "#implementing Bag Of Words\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#cv = CountVectorizer()\n",
    "\n",
    "# insted of selecting all the text as column we have to use the most frequent column. insted of taking all the words i \n",
    "# take to 5000 most frequent words\n",
    "cv = CountVectorizer(max_features=5000)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "\n",
    "\n",
    "# get_dummies() function is used to convert categorical variable into dummy/indicator variables.\n",
    "\n",
    "y = pd.get_dummies(messages['Label']) \n",
    "print(y.head()) # like one hot encoding. so we use only 1 column because of dummy variable trap\n",
    "y=y.iloc[:,1].values\n",
    "\n",
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20 , random_state = 0)\n",
    "\n",
    "# training model using Naive Bayes Classifier(Multinomial)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_detect_model = MultinomialNB().fit(X_train,y_train)\n",
    "\n",
    "y_pred = spam_detect_model.predict(X_test)\n",
    "y_pred\n",
    "\n",
    "# Comparing y_test and y_pred \n",
    "# using confusion matrix\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_m = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "print('\\n confusion Matrix \\n',confusion_m)\n",
    "\n",
    "# Caluclating Accuracy\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "\n",
    "print(' \\n accuracy',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
